{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/rl_spotifyrecs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_states = [-1, 0, 1]\n",
    "items = [-1, 1]\n",
    "slates = list(product(items, repeat=4))\n",
    "num_actions=len(slates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(u,r):\n",
    "    rew=0\n",
    "    for j in range(len(r)):\n",
    "        if u!=0:\n",
    "            rew+=u*r[j]\n",
    "        else:\n",
    "            rew+=0.0\n",
    "    return rew\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(current_state, action):\n",
    "    # Update the new state based on the action\n",
    "    new_state = current_state + sum(action)\n",
    "    \n",
    "    # Clip the new state to ensure it stays within the defined states (1, -1, 0)\n",
    "    new_state = max(min(new_state, 1), -1)\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((len(user_states), len(slates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(Q_table, current_state, slate_action, reward_value, next_state,alpha, gamma):\n",
    "    # Calculate the Q-value for the current state-action pair\n",
    "    action=slates.index(slate_action)\n",
    "    state=user_states.index(current_state)\n",
    "    current_q_value = Q_table[state, action]\n",
    "\n",
    "    # Find the maximum Q-value for the next state and all possible actions\n",
    "    next_state = user_states.index(next_state)\n",
    "    max_next_q_value = np.max(Q_table[next_state, :])\n",
    "\n",
    "    # Update the Q-value using the Q-learning formula\n",
    "    \n",
    "    new_q_value = current_q_value + alpha * (reward_value + gamma * max_next_q_value - current_q_value)\n",
    "\n",
    "    # Update the Q-table with the new Q-value\n",
    "    Q_table[state, action] = new_q_value\n",
    "\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q_table, current_state, epsilon):\n",
    "    state=user_states.index(current_state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Choose a random action\n",
    "\n",
    "        i = np.random.choice(num_actions)\n",
    "        action = slates[i]\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        i = np.argmax(Q_table[state])\n",
    "        action = slates[i]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(Q_table, transition_function, reward, update_q_value, num_episodes, alpha, gamma, epsilon):\n",
    "    for episode in range(num_episodes):\n",
    "        # Randomly choose an initial state\n",
    "        current_state = np.random.choice(user_states)\n",
    "        \n",
    "        while True:\n",
    "            # Choose an action using epsilon-greedy strategy\n",
    "            selected_action = epsilon_greedy_action(Q_table, current_state, epsilon)\n",
    "            action=slates.index(selected_action)\n",
    "            # Transition to the next state based on the chosen action\n",
    "            next_state = transition_function(current_state, selected_action)\n",
    "            \n",
    "            # Assume a reward for the transition (you should replace this with the actual reward from your environment)\n",
    "            reward_value = reward(current_state,selected_action) # Replace with the actual reward\n",
    "            \n",
    "            # Update the Q-value based on the transition\n",
    "            Q_table = update_q_value(Q_table, current_state, selected_action, reward_value, next_state,alpha, gamma)\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # Terminate the episode if a terminal state is reached (you should replace this with your termination condition)\n",
    "            if current_state in [-1, 1]:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "run_episodes(Q_table, transition_function, reward, update_q_value, num_episodes, alpha, gamma, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "For user state -1: optimal slate is (-1, -1, -1, -1) and (-1, 1, -1, -1) \n",
      "0 13\n",
      "For user state 0: optimal slate is (-1, -1, -1, -1) and (1, 1, -1, 1) \n",
      "15 7\n",
      "For user state 1: optimal slate is (1, 1, 1, 1) and (-1, 1, 1, 1) \n"
     ]
    }
   ],
   "source": [
    "first_max = np.argmax(Q_table, axis=1)\n",
    "second_max= np.argsort(Q_table, axis=1)[:,-2]\n",
    "# Print the result\n",
    "for i in range(len(first_max)):\n",
    "    print(first_max[i],second_max[i])\n",
    "    print(f\"For user state {user_states[i]}: optimal slate is {slates[first_max[i]]} and {slates[second_max[i]]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.        ,  9.2482646 ,  9.35905099,  8.92934838,  9.44416439,\n",
       "         8.58028066,  8.52216016,  8.49047049,  9.41622888,  8.73392176,\n",
       "         8.74951174,  8.40197068,  8.9011353 ,  8.34902652,  8.45464298,\n",
       "         7.91071345],\n",
       "       [ 9.        ,  8.92265867,  8.92019606,  8.01080498,  8.90464382,\n",
       "         7.92567025,  7.89626084,  8.92819625,  8.47663113,  8.04214741,\n",
       "         8.05261731,  8.95683373,  8.03549523,  8.97365755,  8.91209385,\n",
       "         8.96949394],\n",
       "       [ 7.88963102,  8.463682  ,  8.33352851,  8.69011501,  8.433103  ,\n",
       "         8.7189583 ,  8.83638412,  9.44715532,  8.41911709,  8.90637813,\n",
       "         8.91155333,  9.4077317 ,  8.89048478,  9.22678888,  9.37855277,\n",
       "        10.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
