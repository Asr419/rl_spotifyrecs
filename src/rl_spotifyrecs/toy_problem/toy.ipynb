{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_states = [-1, 0, 1]\n",
    "items = [-1, 1]\n",
    "slates = list(product(items, repeat=5))\n",
    "num_actions=len(slates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(u,r):\n",
    "    rew=0\n",
    "    for j in range(len(r)):\n",
    "        if u!=0:\n",
    "            rew+=u*r[j]\n",
    "        else:\n",
    "            rew+=0.0\n",
    "    return rew\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(current_state, action):\n",
    "    # Update the new state based on the action\n",
    "    new_state = current_state + sum(action)\n",
    "    \n",
    "    # Clip the new state to ensure it stays within the defined states (1, -1, 0)\n",
    "    new_state = max(min(new_state, 1), -1)\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((len(user_states), len(slates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q_table, current_state, epsilon):\n",
    "    state=user_states.index(current_state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Choose a random action\n",
    "\n",
    "        i = np.random.choice(num_actions)\n",
    "        action = slates[i]\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        i = np.argmax(Q_table[state])\n",
    "        action = slates[i]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSlate(s=0,otherargs=[]):\n",
    "    i = np.random.choice(num_actions)\n",
    "    action = slates[i]\n",
    "   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearned(s=0,qtable=[]):\n",
    "    st = user_states.index(s)\n",
    "    action_indx = np.argmax(qtable[st,:])\n",
    "    slate = slates[action_indx]\n",
    "    return slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(Q_table,serveSlate=randSlate, num_episodes=1000, session_length=10, transition_function=transition_function, reward=reward,epsilon=0.1):\n",
    "    training=[]\n",
    "    cumr=0\n",
    "    for episode in range(num_episodes):\n",
    "        # Randomly choose an initial state\n",
    "        current_state = np.random.choice(user_states)\n",
    "        sess_reward=0\n",
    "        \n",
    "        for sess in range(session_length):\n",
    "            \n",
    "            # Choose an action using epsilon-greedy strategy\n",
    "            selected_action = serveSlate(current_state,Q_table)\n",
    "            action=slates.index(selected_action)\n",
    "            # Transition to the next state based on the chosen action\n",
    "            next_state = transition_function(current_state, selected_action)\n",
    "            \n",
    "            # Assume a reward for the transition (you should replace this with the actual reward from your environment)\n",
    "            reward_value = reward(current_state,selected_action)\n",
    "            sess_reward+=reward_value # Replace with the actual reward\n",
    "            training.append([current_state,selected_action,next_state,reward_value])\n",
    "            # Update the Q-value based on the transition\n",
    "            # Q_table = update_q_value(Q_table, current_state, selected_action, reward_value, next_state,alpha, gamma)\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # Terminate the episode if a terminal state is reached (you should replace this with your termination condition)\n",
    "        cumr+=sess_reward\n",
    "    return cumr, training   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(policy, Qtable =[],numSamples=100, numTrans=2000):\n",
    "    c = 0\n",
    "    for i in range(numSamples):\n",
    "        cum_r, train = run_episodes(Q_table,policy,numTrans)\n",
    "        c = c+cum_r\n",
    "    \n",
    "    return c/numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(Q_table, training, alpha, gamma, test=False):\n",
    "    # Calculate the Q-value for the current state-action pair\n",
    "    t=0\n",
    "    for sample in training:\n",
    "        current_state,slate_action,next_state,reward_value=sample\n",
    "        action=slates.index(slate_action)\n",
    "        state=user_states.index(current_state)\n",
    "        current_q_value = Q_table[state, action]\n",
    "\n",
    "    # Find the maximum Q-value for the next state and all possible actions\n",
    "        next_state = user_states.index(next_state)\n",
    "        max_next_q_value = np.max(Q_table[next_state, :])\n",
    "\n",
    "    # Update the Q-value using the Q-learning formula\n",
    "    \n",
    "        new_q_value = current_q_value + alpha * (reward_value + gamma * max_next_q_value - current_q_value)\n",
    "\n",
    "    # Update the Q-table with the new Q-value\n",
    "        Q_table[state, action] = new_q_value\n",
    "        if test and (t%500)==0:\n",
    "            v=evaluatePolicy(qLearned,Q_table)\n",
    "            action=np.argmax(Q_table,axis=1)\n",
    "            print(action, end='')\n",
    "            print(t, '\\t',v)\n",
    "        t=t+1\n",
    "\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_r, training=run_episodes(Q_table,randSlate, 100, 10, transition_function, reward, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]0 \t 89999.3\n",
      "[ 0  3 31]500 \t 96657.7\n"
     ]
    }
   ],
   "source": [
    "qtable = q_learning(Q_table, training, 0.01, 0.9,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 23, 31])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8\n",
      "For user state -1: optimal slate is (-1, -1, -1, -1, -1) and (-1, 1, -1, -1, -1) \n",
      "23 1\n",
      "For user state 0: optimal slate is (1, -1, 1, 1, 1) and (-1, -1, -1, -1, 1) \n",
      "31 23\n",
      "For user state 1: optimal slate is (1, 1, 1, 1, 1) and (1, -1, 1, 1, 1) \n"
     ]
    }
   ],
   "source": [
    "first_max = np.argmax(Q_table, axis=1)\n",
    "second_max= np.argsort(Q_table, axis=1)[:,-2]\n",
    "# Print the result\n",
    "for i in range(len(first_max)):\n",
    "    print(first_max[i],second_max[i])\n",
    "    print(f\"For user state {user_states[i]}: optimal slate is {slates[first_max[i]]} and {slates[second_max[i]]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69591718,  0.22880015,  0.34921306,  0.1050774 ,  0.50424737,\n",
       "         0.21871536,  0.22819879, -0.12851021,  0.55948606,  0.17656434,\n",
       "         0.15811515, -0.10352437,  0.14527352, -0.12060689, -0.05796252,\n",
       "        -0.18539161,  0.38645638,  0.22609747,  0.10990446, -0.0764675 ,\n",
       "         0.08264216, -0.07569952, -0.09418078, -0.29762767,  0.14472183,\n",
       "        -0.05698649, -0.15456403, -0.32899376, -0.11158001, -0.41384508,\n",
       "        -0.30058442, -0.4440331 ],\n",
       "       [ 0.01587236,  0.03778851,  0.01997979,  0.0325706 ,  0.0232588 ,\n",
       "         0.03016652,  0.01881125,  0.01937365,  0.01766536,  0.01447553,\n",
       "         0.02023201,  0.01687905,  0.01228107,  0.02291382,  0.01383133,\n",
       "         0.02891615,  0.01604562,  0.02101945,  0.01172297,  0.027377  ,\n",
       "         0.0235266 ,  0.03072435,  0.01507656,  0.04009962,  0.02322299,\n",
       "         0.03032662,  0.02451829,  0.01825815,  0.02504339,  0.01644053,\n",
       "         0.02173083,  0.02326127],\n",
       "       [-0.44728369, -0.26883099, -0.41629955, -0.10336648, -0.18318866,\n",
       "        -0.11235099, -0.17973458,  0.13570127, -0.32497395, -0.07633513,\n",
       "        -0.12079213,  0.14093269, -0.17137005,  0.11380427,  0.23854886,\n",
       "         0.32053383, -0.21264862, -0.07640308, -0.07637028,  0.09795659,\n",
       "        -0.10342714,  0.19436255,  0.15109841,  0.48776599, -0.09501825,\n",
       "         0.12470587,  0.18564387,  0.4128362 ,  0.10324655,  0.47031066,\n",
       "         0.34672639,  0.64831737]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
