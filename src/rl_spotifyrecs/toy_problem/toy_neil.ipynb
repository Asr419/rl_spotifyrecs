{"cells":[{"cell_type":"code","execution_count":1,"id":"3e5d7b95","metadata":{},"outputs":[],"source":["import numpy as np\n","import random"]},{"cell_type":"code","execution_count":2,"id":"32d179f5","metadata":{},"outputs":[],"source":["# A simple RL sequence recommendation model. There are 3 user states {-1, 0, 1} and two possible items {-1,1}\n","# A sequence is recommended to the user, who considers each item in turn and either skips it or consumes it\n","\n","# After each item interaction, the system may transition to a new state, at which point a \n","# new sequence is recommended to the user. The system must transition after N steps, if a transition has \n","# not occurred earlier.\n","\n","# A recommendation is an N-length slate of items, such as [-1,-1,1,1] when N=4\n"]},{"cell_type":"code","execution_count":3,"id":"cddd8005","metadata":{},"outputs":[],"source":["# slate size\n","\n","N = 5\n","\n","# state space size\n","\n","S = 3"]},{"cell_type":"code","execution_count":4,"id":"5ec8eb5c","metadata":{},"outputs":[],"source":["# transition probability\n","\n","# this is the probability that the system transitions to a new user state after interacting with\n","# an item\n","\n","ps = 0.3"]},{"cell_type":"code","execution_count":5,"id":"5ba43379","metadata":{},"outputs":[],"source":["# choose which skip model to use, as described in next cell\n","skipmodel = 2"]},{"cell_type":"code","execution_count":6,"id":"1f9df9bb","metadata":{},"outputs":[],"source":["# skip probabilities\n","\n","# here we define a model of the probability that an item will be skipped\n","# depending on the history of items that have been interacted with so far\n","\n","# \n","\n","if skipmodel ==1:\n","\n"," pskip = 0.1; # probability of a skip when state=0\n"," pskippospos = 0.0; # probability of a skip when state=1 and item=1\n"," pskipnegneg = 0.0; # probability of a skip when state=-1 and item=-1\n"," pskipposneg = 0.2; # probability of a skip when state=1 and item=-1\n"," pskipnegpos = 0.2; # probability of a skip when state=-1 and item=1\n"," \n","if skipmodel == 2:\n"," pskip = 0.1; # probability of a skip when state=0\n"," pskippospos = 0.0; # probability of a skip when state=1 and item=1\n"," pskipnegneg = 0.1; # probability of a skip when state=-1 and item=-1\n"," pskipposneg = 0.2; # probability of a skip when state=1 and item=-1\n"," pskipnegpos = 0.2; # probability of a skip when state=-1 and item=1\n","\n","\n","\n","\n","# adjust these probabilities, depending on a history of two past items\n","# skipProb[state, item, prev_item, prev_prev_item]\n","# (This is just completely arbitrary - no basis for the formulae used below)\n","\n","\n","skipProb = np.zeros([3, 2, 2, 2])\n","\n","\n","skipProb[0,0,0,0] = 1-(1-pskipnegneg)**2\n","skipProb[0,0,0,1] = 1-(1-pskipnegneg)\n","skipProb[0,0,1,0] = 1-(1-pskipnegneg)**0.5\n","skipProb[0,0,1,1] = 1-(1-pskipnegneg)**0.25\n","skipProb[0,1,0,0] = 1-(1-pskipnegpos)**0.25\n","skipProb[0,1,0,1] = 1-(1-pskipnegpos)**0.5\n","skipProb[0,1,1,0] = 1-(1-pskipnegpos)\n","skipProb[0,1,1,1] = 1-(1-pskipnegpos)**2\n","skipProb[1,0,0,0] = 1-(1-pskip)**2\n","skipProb[1,0,0,1] = 1-(1-pskip)\n","skipProb[1,0,1,0] = 1-(1-pskip)**0.5\n","skipProb[1,0,1,1] = 1-(1-pskip)**0.25\n","skipProb[1,1,0,0] = 1-(1-pskip)**0.25\n","skipProb[1,1,0,1] = 1-(1-pskip)**0.5\n","skipProb[1,1,1,0] = 1-(1-pskip)\n","skipProb[1,1,1,1] = 1-(1-pskip)**2\n","skipProb[2,0,0,0] = 1-(1-pskipposneg)**2\n","skipProb[2,0,0,1] = 1-(1-pskipposneg)\n","skipProb[2,0,1,0] = 1-(1-pskipposneg)**0.5\n","skipProb[2,0,1,1] = 1-(1-pskipposneg)**0.25\n","skipProb[2,1,0,0] = 1-(1-pskippospos)**0.25\n","skipProb[2,1,0,1] = 1-(1-pskippospos)**0.5\n","skipProb[2,1,1,0] = 1-(1-pskippospos)\n","skipProb[2,1,1,1] = 1-(1-pskippospos)**2\n"]},{"cell_type":"code","execution_count":7,"id":"0891896a","metadata":{},"outputs":[],"source":["# map a slate to an index in the range [0,2**N)\n","\n","def indexOfSlate(slate):\n","    slate = (slate+1)/2\n","    N = len(slate)\n","    indx = 0\n","    for k in range(N):\n","        indx = indx + slate[k]*2**(N-k-1)\n","\n","    return int(indx)\n"," "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["15"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["indexOfSlate(np.array([-1,1,1,1,1]))"]},{"cell_type":"code","execution_count":9,"id":"8a21e59e","metadata":{},"outputs":[],"source":["# map an index in the range [0,2**N) back to the corresponding slate\n","def slateOfIndex(indx,N):\n","\n","    slate = []\n","    \n","    for i in range(N):\n","        slate = [indx%2] + slate\n","        indx = indx//2\n","    slate = np.array(slate)\n","    slate = 2*slate-1\n","    return slate.astype(int)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["array([-1, -1,  1, -1, -1])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["slateOfIndex(4,5)"]},{"cell_type":"code","execution_count":121,"id":"eccaab53","metadata":{},"outputs":[],"source":["# map an index in the range [0,num_states) to the corresponding state\n","\n","def stateOfIndex(indx):\n","    state = indx - 1\n","    return state"]},{"cell_type":"code","execution_count":122,"id":"aac298d0","metadata":{},"outputs":[],"source":["# map a state to an index in the range [0,num_states). \n","\n","def indexOfState(state):\n","    indx = state + 1\n","    return indx"]},{"cell_type":"markdown","id":"f908140d","metadata":{},"source":["# User state generation functions"]},{"cell_type":"code","execution_count":123,"id":"4525a855","metadata":{},"outputs":[],"source":["\n","def randState():\n"," return 2*round(random.random())-1\n"]},{"cell_type":"markdown","id":"607ea23f","metadata":{},"source":["# Slate Generation functions"]},{"cell_type":"code","execution_count":124,"id":"c564f7f0","metadata":{},"outputs":[],"source":["def randSlate(N,s=0,otherargs=[]):\n","    slate = 2*np.round(np.random.random(N))-1\n","    return slate.astype(int)"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"data":{"text/plain":["array([ 1,  1, -1,  1,  1])"]},"execution_count":125,"metadata":{},"output_type":"execute_result"}],"source":["randSlate(5)"]},{"cell_type":"code","execution_count":126,"id":"e998d8ed","metadata":{},"outputs":[],"source":["def posSlate(N,s=0,otherargs=[]):\n","    slate = np.ones(N).astype(int)\n","    return slate"]},{"cell_type":"code","execution_count":127,"id":"03aed325","metadata":{},"outputs":[],"source":["def negSlate(N,s=0,otherargs=[]):\n","    slate = -np.ones(N).astype(int)\n","    return slate"]},{"cell_type":"code","execution_count":128,"id":"623f78c5","metadata":{},"outputs":[],"source":["def qLearned(N,s=0,qtable=[]):\n","    st = indexOfState(s)\n","    action_indx = np.argmax(qtable[st,:])\n","    slate = slateOfIndex(action_indx,N)\n","    return slate"]},{"cell_type":"markdown","id":"c4990b9a","metadata":{},"source":["# Reward functions"]},{"cell_type":"code","execution_count":129,"id":"d1befd8a","metadata":{},"outputs":[],"source":["# reward = 1 if the state == item (action) and 0 otherwise\n","def oddsReward(s, action,k,skip):\n","    if (~skip and s == action[k]):\n","        reward = 1\n","    else:\n","        reward = 0\n"," \n","    return reward\n"]},{"cell_type":"markdown","id":"bccfda52","metadata":{},"source":["# Transition functions"]},{"cell_type":"code","execution_count":130,"id":"29042294","metadata":{},"outputs":[],"source":["# If an item is consumed, then if state != 0, state = 0, otherwise state = consumed item\n","# Idea is that user has consumed an item, then is not intereted to consume again on next round\n","\n","# Want an interesting transition function that yields long term value ... experiment more here\n","\n","def doTransition(s, slate, k, skip):\n","    state = s\n","    if (~skip):\n","        if (s==slate[k] or s == -slate[k]):\n","            state = 0\n","\n","    if (s==0):\n","        state = slate[k]\n","    \n","    return state"]},{"cell_type":"markdown","id":"395cce0f","metadata":{},"source":["# Skip functions"]},{"cell_type":"code","execution_count":131,"id":"da499aaa","metadata":{},"outputs":[],"source":["# Determine whether or not to skip. Use the skipProb model - if haven't sufficient items in the history, then\n","# average over the missing history slots\n","\n","def doSkip(s, slate, k):\n","   \n","   s_indx = indexOfState(s)\n","   slate_index = ((slate+1)/2).astype(int)\n","\n","   r = random.random()\n","   \n","   if (k==0):\n","      testp = np.mean(np.mean(skipProb[s_indx,slate_index[k],:,:]))\n","   elif (k==1):\n","      testp = np.mean(skipProb[s_indx,slate_index[k],slate_index[k-1],:])\n","   else:\n","      testp = skipProb[s_indx,slate_index[k],slate_index[k-1],slate_index[k-2]]\n","   \n","   \n","   skip = r < testp\n","      \n","   return skip, testp\n"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[{"data":{"text/plain":["(False, 0.2799999999999999)"]},"execution_count":132,"metadata":{},"output_type":"execute_result"}],"source":["doSkip(-1,np.array([1,1,1,1,1]),1)"]},{"cell_type":"markdown","id":"ab699438","metadata":{},"source":["# Game"]},{"cell_type":"code","execution_count":133,"id":"d2cc25fb","metadata":{},"outputs":[],"source":["# play the game, returning the cumulative reward over T state transitions. Also, return the \n","# training data, consisting of the (s, a, r, s') tuples required to train a q-learning algorithm\n","\n","# Arguements:\n","\n","# serveSlate = function used to generate a slate\n","# T = number of transitions\n","# N = size of slate\n","# qtable = a table of q-values, which is used when using a Q-table to select the next slate\n","# randomState = function to generate a random state\n","# rewardFunction = function to generate the reward\n","# transitionFunction = function to select the next state to transition to\n","# skipFunction = function to determine if an item is skipped or not\n","\n","\n","def game(serveSlate=randSlate,T=1000, N=3, qtable=np.zeros([3,2**N]),\n","         randomState=randState, rewardFunction=oddsReward, transitionFunction=doTransition,\n","         skipFunction=doSkip):\n","\n","\n","    randomState = randState\n","    rewardFunction = oddsReward\n","    transitionFunction = doTransition\n","    skipFunction = doSkip\n","\n","    cumr = 0\n","    s = randState()\n","\n","    training = []\n","\n","    t = 0\n","    while (t<T):\n","        slate = serveSlate(N,s,qtable)\n","    \n","        slate_reward = 0\n","        r_sample = np.zeros(N)\n","        sk_sample = np.zeros(N)\n","        for k in range(N):\n","            skip,testp = skipFunction(s, slate,k)\n","            \n","            \n","            r = rewardFunction(s, slate,k,skip)\n","            r_sample[k] = r\n","            sk_sample[k] = skip\n","            slate_reward = slate_reward + r\n","            if (random.random()<ps):\n","                break\n","                \n","        cumr = cumr + slate_reward\n","    \n","    \n","        next_s = transitionFunction(s,slate,k,skip)\n","        training.append([indexOfState(s), \n","            indexOfSlate(slate), slate_reward, indexOfState(next_s),\n","            k, sk_sample, r_sample])\n","        \n","        s = next_s\n","        t = t+1\n","        \n","    return cumr, training"]},{"cell_type":"code","execution_count":134,"id":"788ca3c1","metadata":{},"outputs":[],"source":["def evaluatePolicy(policy, Qtable =[],numSamples=100, numTrans=1000):\n","    c = 0\n","    for i in range(numSamples):\n","        cum_r, train = game(policy,numTrans,N,Qtable)\n","        c = c+cum_r\n","    \n","    return c/numSamples\n","    "]},{"cell_type":"markdown","id":"0933448e","metadata":{},"source":["# TD Q-learning over slates"]},{"cell_type":"code","execution_count":135,"id":"f8d15fcc","metadata":{},"outputs":[],"source":["def qlearning(N, S, training, alpha, gamma, test=False):\n","\n","    # N = slate size\n","    # S = size of state space\n","\n","    # size of action space\n","    A = 2**N\n","\n","\n","    qtable = np.random.random([S, A])\n","\n","    t = 0\n","    for sample in training:\n","    \n","        curr_s = sample[0]\n","        action = sample[1]\n","        reward = sample[2]\n","        next_s = sample[3]\n","    \n","        qtable[curr_s, action] = (1-alpha)*qtable[curr_s,action] + \\\n","            alpha*(reward + gamma*np.max(qtable[next_s,:]))\n","                        \n","        if test and (t % 500) == 0 :\n","            \n","            v = evaluatePolicy(qLearned,qtable)     \n","            action =np.argmax(qtable,axis=1)\n","            print(action, end=' ')\n","            print(t,'\\t', v)\n","\n","        t = t+1\n","                    \n","                    \n","    return qtable"]},{"cell_type":"code","execution_count":136,"id":"45d0efff","metadata":{},"outputs":[],"source":["def slateExpectedValue(s_indx,slate_indx,N,qtable,qtableN):\n","\n","    slate = slateOfIndex(slate_indx,N)\n","    s = stateOfIndex(s_indx)\n","    slate_q_value = 0\n","    qt = qtable\n","    for i in range(N):\n","        if (i==N-1):\n","            qt = qtableN\n","\n","        slate_indx = indexOfSlate(np.array([slate[i]]))\n","        slate_indx_skip = slate_indx + 2\n","    \n","        t,skip_prob = doSkip(s, slate,i)\n","        slate_q_value = slate_q_value + \\\n","            ((1-ps)**(i))*((1-skip_prob)*qt[s_indx,slate_indx] + skip_prob*qt[s_indx,slate_indx_skip])\n","        \n","    return slate_q_value"]},{"cell_type":"markdown","id":"e35ae724","metadata":{},"source":["# Itemwise TD Q-learning using Expected value of Slate"]},{"cell_type":"code","execution_count":137,"id":"8244fa5d","metadata":{},"outputs":[],"source":["def slateQLearning(N, S, training, alpha, gamma,test=False):\n","\n","\n","    # N = slate size\n","    # S = size of state space\n","\n","    # size of action space\n","    A = 2*2\n","\n","\n","    Qtable = np.random.random([S, A])\n","    QtableN = Qtable\n","    Qslate = np.zeros([S,2**N])\n","\n","    t = 0\n","    for sample in training:\n","    \n","    \n","        curr_s = sample[0]\n","        slate_indx = sample[1]\n","        slate_reward = sample[2]\n","        next_s = sample[3]\n","        trans_pos = sample[4]\n","        skip_list = sample[5]\n","        r_list = sample[6]\n","    \n","        slate = slateOfIndex(slate_indx,N)\n","    \n","        for k in range(trans_pos):\n","            item = indexOfSlate(np.array([slate[k]]))\n","            r = r_list[k]\n","            skip = skip_list[k]\n","    \n","            if (skip):\n","                item = item + 2\n","\n","        \n","            for i in range(2**N):\n","                Qslate[curr_s,i] = slateExpectedValue(curr_s,i,N,Qtable,QtableN)\n","\n","            Qtable[curr_s, item] = (1-alpha)*Qtable[curr_s,item] + \\\n","                alpha*(r + ps*gamma*np.max(Qslate[curr_s,:]))\n","\n","\n","    \n","        k = trans_pos\n","        item = indexOfSlate(np.array([slate[k]]))\n","        r = r_list[k]\n","        skip = skip_list[k]\n","        if (skip):\n","            item = item + 2\n","    \n","        for i in range(2**N):\n","            Qslate[next_s,i] = slateExpectedValue(next_s,i,N,Qtable,QtableN)\n","    \n","        if k<N-1:\n","  \n","            Qtable[curr_s, item] = (1-alpha)*Qtable[curr_s,item] + \\\n","                alpha*(r + ps*gamma*np.max(Qslate[next_s,:]))\n","        else:\n","    \n","            QtableN[curr_s, item] = (1-alpha)*QtableN[curr_s,item] + \\\n","                alpha*(r + gamma*np.max(Qslate[next_s,:]))\n","\n","    \n","        if test and (t % 500) == 0 :\n","            \n","            v = evaluatePolicy(qLearned,Qtable)     \n","            action =np.argmax(Qslate,axis=1)\n","            print(action, end=' ')\n","            print(t,'\\t', v)\n","\n","        t = t+1\n","    \n","    return Qtable, QtableN, Qslate"]},{"cell_type":"markdown","id":"8b8678aa","metadata":{},"source":["# Expected value of a Slate in terms of Itemwise Q-values"]},{"cell_type":"code","execution_count":null,"id":"680fd414","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b127de7f","metadata":{},"source":["# Experiments"]},{"cell_type":"code","execution_count":138,"id":"9fb920de","metadata":{},"outputs":[],"source":["cum_r, training = game(randSlate,10000)"]},{"cell_type":"code","execution_count":139,"id":"e7351664","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Value of Neg Policy 1282.9\n"]}],"source":["# quality when we always recommend item = -1\n","\n","print('Value of Neg Policy', evaluatePolicy(negSlate))"]},{"cell_type":"code","execution_count":140,"id":"fb7f0a8e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Value of Pos Policy 1384.92\n"]}],"source":["# quality when we always recommend item = 1\n","\n","\n","print('Value of Pos Policy', evaluatePolicy(posSlate))"]},{"cell_type":"code","execution_count":141,"id":"9e132558","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[18  4  6] 0 \t 668.58\n","[18  9  6] 500 \t 588.45\n","[0 9 6] 1000 \t 908.24\n","[0 9 6] 1500 \t 905.52\n","[0 9 7] 2000 \t 965.6\n","[0 9 7] 2500 \t 962.98\n","[0 9 7] 3000 \t 972.59\n","[0 9 7] 3500 \t 967.65\n","[0 9 7] 4000 \t 970.42\n","[0 9 7] 4500 \t 973.91\n","[0 9 7] 5000 \t 964.81\n","[0 9 7] 5500 \t 961.75\n","[0 9 7] 6000 \t 970.61\n","[0 9 7] 6500 \t 969.27\n","[0 9 7] 7000 \t 968.4\n","[0 9 7] 7500 \t 963.22\n","[0 9 7] 8000 \t 964.41\n","[0 9 7] 8500 \t 962.46\n","[0 9 7] 9000 \t 968.13\n","[0 9 7] 9500 \t 968.45\n"]}],"source":["qtable = qlearning(N, S, training, 0.01, 0.1,True)"]},{"cell_type":"code","execution_count":142,"id":"b16b1ddb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0 31  0] 0 \t 816.46\n","[31 31  0] 500 \t 817.61\n","[31 31  0] 1000 \t 760.81\n","[31 31  0] 1500 \t 758.25\n","[31 31  0] 2000 \t 644.77\n","[31 31 22] 2500 \t 727.9\n","[31 31 31] 3000 \t 803.09\n","[31 31 22] 3500 \t 733.59\n","[22 31 27] 4000 \t 823.16\n","[31 31 31] 4500 \t 728.6\n","[31 31 31] 5000 \t 728.33\n","[27 31 22] 5500 \t 722.06\n","[27 31 22] 6000 \t 733.46\n","[21 31 22] 6500 \t 812.94\n","[27 31 22] 7000 \t 727.69\n","[21 31 22] 7500 \t 813.97\n","[22 31 22] 8000 \t 725.94\n","[10 31 31] 8500 \t 810.87\n","[10 31 22] 9000 \t 815.75\n","[10 31 22] 9500 \t 814.09\n"]}],"source":["Qtable, QtableN, Qslate = slateQLearning(N,S,training,0.01,0.1,True)"]},{"cell_type":"code","execution_count":null,"id":"bfdfa464","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2cbffb04","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
