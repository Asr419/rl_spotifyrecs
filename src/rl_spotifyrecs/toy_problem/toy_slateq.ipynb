{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_states = [-1, 0, 1]\n",
    "items = [-1, 1]\n",
    "slates = list(product(items, repeat=5))\n",
    "num_actions=len(slates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_slateq(u,r):\n",
    "    rew=u*r\n",
    "    return rew\n",
    "def reward(u,r):\n",
    "    rew=0\n",
    "    for j in range(len(r)):\n",
    "        if u!=0:\n",
    "            rew+=u*r[j]\n",
    "        else:\n",
    "            rew+=0.0\n",
    "    if rew>=0:\n",
    "        rew=1\n",
    "    elif rew==0:\n",
    "        rew=0\n",
    "    else:\n",
    "        rew=-1\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function_slateq(current_state, action):\n",
    "    # Update the new state based on the action\n",
    "    if action == None:\n",
    "        new_state = 0\n",
    "    else:\n",
    "        new_state = action\n",
    "    return new_state\n",
    "def transition_function(current_state, action):\n",
    "    # Update the new state based on the action\n",
    "    new_state = current_state + sum(action)\n",
    "    \n",
    "    # Clip the new state to ensure it stays within the defined states (1, -1, 0)\n",
    "    new_state = max(min(new_state, 1), -1)\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_i_u = np.zeros((len(user_states), len(items)))\n",
    "for i in range(len(user_states)):\n",
    "    for j in range(len(items)):\n",
    "        if i == j:\n",
    "            prob_i_u[i, j] = 0.6\n",
    "        else:\n",
    "            prob_i_u[i, j] = random.uniform(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_item = np.zeros((len(user_states), len(items)))\n",
    "Q_slate = np.zeros((len(user_states), len(slates)))\n",
    "Q_table = np.zeros((len(user_states), len(slates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSlate(s=0,otherargs=[]):\n",
    "    i = np.random.choice(num_actions)\n",
    "    action = slates[i]\n",
    "   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearned(s=0,qtable=[]):\n",
    "    st = user_states.index(s)\n",
    "    action_indx = np.argmax(qtable[st,:])\n",
    "    slate = slates[action_indx]\n",
    "    return slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_q_value_slateq(Q_item,Q_slate, current_state, selected_item, slate, reward_value, next_state,alpha, gamma):\n",
    "#     state=user_states.index(current_state)\n",
    "#     action=slates.index(slate)\n",
    "#     if selected_item!=0:\n",
    "#         selected_item_index=items.index(selected_item)\n",
    "#         current_q_value=Q_item[state,selected_item_index]\n",
    "#         next_state=user_states.index(next_state)\n",
    "#         max_next_q_value=np.max(Q_item[next_state,:])\n",
    "#         new_q_value=current_q_value+alpha*(reward_value+gamma*max_next_q_value-current_q_value)\n",
    "#         Q_item[state,selected_item_index]=new_q_value\n",
    "    \n",
    "#     for i in range(len(slate)):\n",
    "#         item_action=items.index(slate[i])\n",
    "#         Q_slate[state,action]+=Q_item[state,item_action]*prob_i_u[state,item_action]\n",
    "\n",
    "#     return Q_item, Q_slate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_slateq(Q_slate, current_state, epsilon):\n",
    "    state=user_states.index(current_state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Choose a random action\n",
    "\n",
    "        i = np.random.choice(num_actions)\n",
    "        action = slates[i]\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        i = np.argmax(Q_slate[state])\n",
    "        action = slates[i]\n",
    "    return action\n",
    "def epsilon_greedy_action(Q_table, current_state, epsilon):\n",
    "    state=user_states.index(current_state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Choose a random action\n",
    "\n",
    "        i = np.random.choice(num_actions)\n",
    "        action = slates[i]\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        i = np.argmax(Q_table[state])\n",
    "        action = slates[i]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_values = exp_x / np.sum(exp_x)\n",
    "    return softmax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_slateq(Q_slateq,serveSlate=randSlate,num_episodes=1000,  session_length=10, transition_function_slateq=transition_function_slateq, reward_slateq=reward_slateq, epsilon=0.01):\n",
    "    training=[]\n",
    "    cumr=0\n",
    "    for episode in range(num_episodes):\n",
    "        # Randomly choose an initial state\n",
    "        current_state = np.random.choice(user_states)\n",
    "        sess_reward=0\n",
    "        \n",
    "        for sess in range(session_length):\n",
    "            # Choose an action using epsilon-greedy strategy\n",
    "            selected_slate = serveSlate(current_state,Q_slateq)\n",
    "            prob_selection=[]\n",
    "            for i in range(len(selected_slate)):\n",
    "                prob_selection.append(prob_i_u[user_states.index(current_state),items.index(selected_slate[i])])\n",
    "            prob_selection.append(0.2)\n",
    "            prob_selection=softmax(prob_selection)\n",
    "            selected_index = np.random.choice(len(prob_selection), p=prob_selection)\n",
    "            if selected_index<= len(selected_slate)-1:\n",
    "                selected_item=selected_slate[selected_index]\n",
    "            else:\n",
    "                selected_item=0 #None\n",
    "            \n",
    "            \n",
    "            next_state = transition_function_slateq(current_state, selected_item)\n",
    "            \n",
    "            # Assume a reward for the transition (you should replace this with the actual reward from your environment)\n",
    "            reward_value = reward_slateq(current_state,selected_item)\n",
    "             # Replace with the actual reward\n",
    "            training.append([current_state,selected_item,selected_slate, reward_value,next_state])\n",
    "            # # Update the Q-value based on the transition\n",
    "            # Q_item,Q_slate = update_q_value_slateq(Q_item,Q_slate, current_state,selected_item,selected_slate, reward_value, next_state,alpha, gamma)\n",
    "            sess_reward+=reward_value\n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # # Terminate the episode if a terminal state is reached (you should replace this with your termination condition)\n",
    "            # if current_state in [-1, 1]:\n",
    "   #     break\n",
    "        cumr+=sess_reward\n",
    "    return cumr, training\n",
    "\n",
    "\n",
    "def run_episodes(Q_table,serveSlate=randSlate, num_episodes=1000, session_length=10, transition_function=transition_function, reward=reward,epsilon=0.1):\n",
    "    training=[]\n",
    "    cumr=0\n",
    "    for episode in range(num_episodes):\n",
    "        # Randomly choose an initial state\n",
    "        current_state = np.random.choice(user_states)\n",
    "        sess_reward=0\n",
    "        \n",
    "        for sess in range(session_length):\n",
    "            \n",
    "            # Choose an action using epsilon-greedy strategy\n",
    "            selected_action = serveSlate(current_state,Q_table)\n",
    "            action=slates.index(selected_action)\n",
    "            # Transition to the next state based on the chosen action\n",
    "            next_state = transition_function(current_state, selected_action)\n",
    "            \n",
    "            # Assume a reward for the transition (you should replace this with the actual reward from your environment)\n",
    "            reward_value = reward(current_state,selected_action)\n",
    "            sess_reward+=reward_value # Replace with the actual reward\n",
    "            training.append([current_state,selected_action,next_state,reward_value])\n",
    "            # Update the Q-value based on the transition\n",
    "            # Q_table = update_q_value(Q_table, current_state, selected_action, reward_value, next_state,alpha, gamma)\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # Terminate the episode if a terminal state is reached (you should replace this with your termination condition)\n",
    "        cumr+=sess_reward\n",
    "    return cumr, training   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(policy, Qt =[],numSamples=100, numTrans=2000):\n",
    "    c = 0\n",
    "    for i in range(numSamples):\n",
    "        cum_r, train = run_episodes(Qt,policy,numTrans)\n",
    "        c = c+cum_r\n",
    "    \n",
    "    return c/numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(Q_table, training, alpha, gamma, test=False):\n",
    "    # Calculate the Q-value for the current state-action pair\n",
    "    t=0\n",
    "    for sample in training:\n",
    "        current_state,slate_action,next_state,reward_value=sample\n",
    "        action=slates.index(slate_action)\n",
    "        state=user_states.index(current_state)\n",
    "        current_q_value = Q_table[state, action]\n",
    "\n",
    "    # Find the maximum Q-value for the next state and all possible actions\n",
    "        next_state = user_states.index(next_state)\n",
    "        max_next_q_value = np.max(Q_table[next_state, :])\n",
    "\n",
    "    # Update the Q-value using the Q-learning formula\n",
    "    \n",
    "        new_q_value = current_q_value + alpha * (reward_value + gamma * max_next_q_value - current_q_value)\n",
    "\n",
    "    # Update the Q-table with the new Q-value\n",
    "        Q_table[state, action] = new_q_value\n",
    "        if test and (t%500)==0:\n",
    "            v=evaluatePolicy(qLearned,Q_table)\n",
    "            action=np.argmax(Q_table,axis=1)\n",
    "            print(action, end='')\n",
    "            print(t, '\\t',v)\n",
    "        t=t+1\n",
    "\n",
    "    return Q_table\n",
    "\n",
    "def slateqLearning(Q_item,Q_slate,training, alpha, gamma,test=False):\n",
    "    t=0\n",
    "    for sample in training:\n",
    "        current_state,selected_item,selected_slate,reward_value,next_state=sample\n",
    "        state=user_states.index(current_state)\n",
    "        action=slates.index(selected_slate)\n",
    "        if selected_item!=0:\n",
    "            selected_item_index=items.index(selected_item)\n",
    "            current_q_value=Q_item[state,selected_item_index]\n",
    "            next_state=user_states.index(next_state)\n",
    "            max_next_q_value=np.max(Q_item[next_state,:])\n",
    "            new_q_value=current_q_value+alpha*(reward_value+gamma*max_next_q_value-current_q_value)\n",
    "            Q_item[state,selected_item_index]=new_q_value\n",
    "        \n",
    "        for i in range(len(selected_slate)):\n",
    "            item_action=items.index(selected_slate[i])\n",
    "            Q_slate[state,action]+=Q_item[state,item_action]*prob_i_u[state,item_action]\n",
    "        if test and (t%500)==0:\n",
    "            v=evaluatePolicy(qLearned,Q_slate)\n",
    "            action=np.argmax(Q_slate,axis=1)\n",
    "            print(action, end='')\n",
    "            print(t, '\\t',v)\n",
    "        t=t+1\n",
    "\n",
    "    return Q_slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_r, training=run_episodes(Q_table,randSlate, 200, 10, transition_function, reward, 0.1)\n",
    "cum_rslate, training_slate=run_episodes_slateq(Q_slate,randSlate, 200, 10, transition_function_slateq, reward_slateq, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]0 \t 18661.84\n",
      "[20 30 14]500 \t 20000.0\n",
      "[10 28  7]1000 \t 20000.0\n",
      "[10 30 30]1500 \t 20000.0\n",
      "[ 8 11 31]499 \t 20000.0\n",
      "[ 4 10 31]999 \t 20000.0\n",
      "[ 2 30 29]1499 \t 20000.0\n",
      "[ 2 30 27]1999 \t 20000.0\n"
     ]
    }
   ],
   "source": [
    "qtable = q_learning(Q_table, training, 0.01, 0.9,True)\n",
    "qslate=slateqLearning(Q_item,Q_slate,training_slate, 0.01, 0.9,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 17\n",
      "SlateQ : For user state -1: optimal slate is (-1, -1, -1, 1, -1) and (1, -1, -1, -1, 1) \n",
      "30 29\n",
      "SlateQ : For user state 0: optimal slate is (1, 1, 1, 1, -1) and (1, 1, 1, -1, 1) \n",
      "27 31\n",
      "SlateQ : For user state 1: optimal slate is (1, 1, -1, 1, 1) and (1, 1, 1, 1, 1) \n",
      "10 8\n",
      "For user state -1: optimal slate is (-1, 1, -1, 1, -1) and (-1, 1, -1, -1, -1) \n",
      "28 30\n",
      "For user state 0: optimal slate is (1, 1, 1, -1, -1) and (1, 1, 1, 1, -1) \n",
      "11 30\n",
      "For user state 1: optimal slate is (-1, 1, -1, 1, 1) and (1, 1, 1, 1, -1) \n"
     ]
    }
   ],
   "source": [
    "first_max_slateq = np.argmax(Q_slate, axis=1)\n",
    "second_max_slateq= np.argsort(Q_slate, axis=1)[:,-2]\n",
    "# Print the result\n",
    "for i in range(len(first_max_slateq)):\n",
    "    print(first_max_slateq[i],second_max_slateq[i])\n",
    "    print(f\"SlateQ : For user state {user_states[i]}: optimal slate is {slates[first_max_slateq[i]]} and {slates[second_max_slateq[i]]} \")\n",
    "\n",
    "first_max = np.argmax(Q_table, axis=1)\n",
    "second_max= np.argsort(Q_table, axis=1)[:,-2]\n",
    "# Print the result\n",
    "for i in range(len(first_max)):\n",
    "    print(first_max[i],second_max[i])\n",
    "    print(f\"For user state {user_states[i]}: optimal slate is {slates[first_max[i]]} and {slates[second_max[i]]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97.06265154, 134.20386477, 200.39180529, 114.51860615,\n",
       "        131.84563733, 128.64697638,  94.84303486,  66.77602982,\n",
       "        108.66970774, 137.21522675,  92.56431787,  55.19487258,\n",
       "         98.91897868,  95.03465239,  40.79988025,  29.90246104,\n",
       "         66.02659434, 148.42295847, 101.16903899,  63.77385998,\n",
       "        116.21032747,  58.24310875,  50.41608215,  33.49603728,\n",
       "         82.7175868 ,  77.4449918 ,  84.8736808 ,  25.44540501,\n",
       "         67.0141909 ,  44.21339077,  32.43722773,   5.18173043],\n",
       "       [  2.3466968 ,   7.25457025,   5.66020792,  15.36259102,\n",
       "          7.44343251,   9.05083127,   4.64930762,  10.28629298,\n",
       "          4.77347788,   5.87055032,   6.26978753,   9.62776109,\n",
       "          5.60290676,  20.21735062,  16.52312703,  10.5930778 ,\n",
       "          5.10199105,  14.65099847,  14.92846088,   8.85435146,\n",
       "         11.2427293 ,  10.46643868,  17.77626836,  14.2315089 ,\n",
       "         13.5320165 ,  10.3047418 ,  14.69926951,  12.89342238,\n",
       "         12.63376872,  21.24401675,  22.09168443,  14.83641661],\n",
       "       [  4.23451575,  51.56315539,  17.4076888 ,  30.59944551,\n",
       "         25.30503422,  51.60067981,  47.99029165,  58.05537552,\n",
       "         32.80652109,  34.71907333,  58.7577153 ,  46.14081064,\n",
       "         39.39299136,  59.05994826,  37.92278238,  77.0680518 ,\n",
       "         20.25742926,  33.87022656,  43.25512311,  53.39916428,\n",
       "         34.68223042,  63.9559027 ,  78.98041321,  67.39111841,\n",
       "         36.01129775,  59.80587405,  42.07580559, 114.357664  ,\n",
       "         57.00107776,  78.68915685,  61.68853396, 106.51067683]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22228268,  0.18961471,  0.27284294,  0.20869637,  0.24407476,\n",
       "         0.24003799,  0.17441526, -0.17393003,  0.30921101,  0.20776096,\n",
       "         0.41317404, -0.21392595,  0.19152398, -0.15884479, -0.17903279,\n",
       "        -0.1924692 ,  0.22859408,  0.2788797 ,  0.30183444, -0.17964276,\n",
       "         0.24846346, -0.15515513, -0.19471306, -0.18481839,  0.28586088,\n",
       "        -0.16194038, -0.17370337, -0.15229613, -0.25242947, -0.1446992 ,\n",
       "        -0.14858333, -0.17335872],\n",
       "       [ 0.17061861,  0.1885507 ,  0.12252921,  0.16842303,  0.18863508,\n",
       "         0.15632043,  0.16778235,  0.15225874,  0.16220515,  0.17948339,\n",
       "         0.15266464,  0.18100349,  0.1548293 ,  0.14651199,  0.19157385,\n",
       "         0.16981999,  0.15352388,  0.13623292,  0.1445608 ,  0.19001307,\n",
       "         0.17480202,  0.18298563,  0.14267529,  0.17921997,  0.11411906,\n",
       "         0.13902975,  0.19262131,  0.17319612,  0.2379022 ,  0.18067681,\n",
       "         0.23318608,  0.13316978],\n",
       "       [-0.15468406, -0.1578489 , -0.19741523, -0.24486317, -0.17026185,\n",
       "        -0.15881391, -0.1848358 ,  0.24254861, -0.13308357, -0.18021628,\n",
       "        -0.1833636 ,  0.31083766, -0.09950035,  0.23828758,  0.20535196,\n",
       "         0.26013703, -0.1837911 , -0.22508413, -0.16133146,  0.273567  ,\n",
       "        -0.26312255,  0.23043638,  0.28031742,  0.22872633, -0.26310862,\n",
       "         0.17274309,  0.22851299,  0.23607782,  0.23375765,  0.20022316,\n",
       "         0.28102539,  0.17944001]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1,  11,   5,   4,   3,   3,   2,   5,  -7,  10, -14,   7,   3,\n",
       "          8,  -1,  -1, -11,   3,  -7,   5,   0,  -3,   5,   0, -13,   4,\n",
       "          8, -12,  13,  -9, -11,  -9],\n",
       "       [-17, -17,   4,  11, -17,  -1, -13,   5, -11, -15,  -2, -11,  -7,\n",
       "         22,  -1,   0,  -7,  19,  19, -17,  -2,  -9,  23,   1,  20,  10,\n",
       "         -6,   1, -13,   8,   1,  22],\n",
       "       [-13,   4,  -3,   2,  -6,   6,  10,  -6,  -9,   0,  14, -17,  -4,\n",
       "         -3,  -9,   0,  -4,   3,   3, -10,   7,   3,   0,   5,   8,   7,\n",
       "         -8,   7,  -4,  10,  -6,  13]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_Q_table = np.apply_along_axis(lambda row: np.argsort(row)[::-1].argsort() + 1, axis=1, arr=Q_table)\n",
    "ranked_Q_slate = np.apply_along_axis(lambda row: np.argsort(row)[::-1].argsort() + 1, axis=1, arr=Q_slate)\n",
    "\n",
    "Ordered_difference = ranked_Q_table - ranked_Q_slate\n",
    "Ordered_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97.06265154, 134.20386477, 200.39180529, 114.51860615,\n",
       "        131.84563733, 128.64697638,  94.84303486,  66.77602982,\n",
       "        108.66970774, 137.21522675,  92.56431787,  55.19487258,\n",
       "         98.91897868,  95.03465239,  40.79988025,  29.90246104,\n",
       "         66.02659434, 148.42295847, 101.16903899,  63.77385998,\n",
       "        116.21032747,  58.24310875,  50.41608215,  33.49603728,\n",
       "         82.7175868 ,  77.4449918 ,  84.8736808 ,  25.44540501,\n",
       "         67.0141909 ,  44.21339077,  32.43722773,   5.18173043],\n",
       "       [  2.3466968 ,   7.25457025,   5.66020792,  15.36259102,\n",
       "          7.44343251,   9.05083127,   4.64930762,  10.28629298,\n",
       "          4.77347788,   5.87055032,   6.26978753,   9.62776109,\n",
       "          5.60290676,  20.21735062,  16.52312703,  10.5930778 ,\n",
       "          5.10199105,  14.65099847,  14.92846088,   8.85435146,\n",
       "         11.2427293 ,  10.46643868,  17.77626836,  14.2315089 ,\n",
       "         13.5320165 ,  10.3047418 ,  14.69926951,  12.89342238,\n",
       "         12.63376872,  21.24401675,  22.09168443,  14.83641661],\n",
       "       [  4.23451575,  51.56315539,  17.4076888 ,  30.59944551,\n",
       "         25.30503422,  51.60067981,  47.99029165,  58.05537552,\n",
       "         32.80652109,  34.71907333,  58.7577153 ,  46.14081064,\n",
       "         39.39299136,  59.05994826,  37.92278238,  77.0680518 ,\n",
       "         20.25742926,  33.87022656,  43.25512311,  53.39916428,\n",
       "         34.68223042,  63.9559027 ,  78.98041321,  67.39111841,\n",
       "         36.01129775,  59.80587405,  42.07580559, 114.357664  ,\n",
       "         57.00107776,  78.68915685,  61.68853396, 106.51067683]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
