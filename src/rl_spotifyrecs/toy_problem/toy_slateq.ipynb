{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_states = [-1, 0, 1]\n",
    "items = [-1, 1]\n",
    "slates = list(product(items, repeat=4))\n",
    "num_actions=len(slates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(u,r):\n",
    "    rew=u*r\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(current_state, action):\n",
    "    # Update the new state based on the action\n",
    "    if action == None:\n",
    "        new_state = 0\n",
    "    else:\n",
    "        new_state = action\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_i_u = np.zeros((len(user_states), len(items)))\n",
    "for i in range(len(user_states)):\n",
    "    for j in range(len(items)):\n",
    "        if i == j:\n",
    "            prob_i_u[i, j] = 0.6\n",
    "        else:\n",
    "            prob_i_u[i, j] = random.uniform(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_item = np.zeros((len(user_states), len(items)))\n",
    "Q_slate = np.zeros((len(user_states), len(slates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(Q_item,Q_slate, current_state, selected_item, slate, reward_value, next_state,alpha, gamma):\n",
    "    state=user_states.index(current_state)\n",
    "    action=slates.index(slate)\n",
    "    if selected_item!=0:\n",
    "        selected_item_index=items.index(selected_item)\n",
    "        current_q_value=Q_item[state,selected_item_index]\n",
    "        next_state=user_states.index(next_state)\n",
    "        max_next_q_value=np.max(Q_item[next_state,:])\n",
    "        new_q_value=current_q_value+alpha*(reward_value+gamma*max_next_q_value-current_q_value)\n",
    "        Q_item[state,selected_item_index]=new_q_value\n",
    "    \n",
    "    for i in range(len(slate)):\n",
    "        item_action=items.index(slate[i])\n",
    "        Q_slate[state,action]+=Q_item[state,item_action]*prob_i_u[state,item_action]\n",
    "\n",
    "    return Q_item, Q_slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q_slate, current_state, epsilon):\n",
    "    state=user_states.index(current_state)\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Choose a random action\n",
    "\n",
    "        i = np.random.choice(num_actions)\n",
    "        action = slates[i]\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        i = np.argmax(Q_slate[state])\n",
    "        action = slates[i]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_values = exp_x / np.sum(exp_x)\n",
    "    return softmax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(Q_item,Q_slate, transition_function, reward, update_q_value, num_episodes, alpha, gamma, epsilon):\n",
    "    for episode in range(num_episodes):\n",
    "        # Randomly choose an initial state\n",
    "        current_state = np.random.choice(user_states)\n",
    "        \n",
    "        while True:\n",
    "            # Choose an action using epsilon-greedy strategy\n",
    "            selected_slate = epsilon_greedy_action(Q_slate, current_state, epsilon)\n",
    "            prob_selection=[]\n",
    "            for i in range(len(selected_slate)):\n",
    "                prob_selection.append(prob_i_u[user_states.index(current_state),items.index(selected_slate[i])])\n",
    "            prob_selection.append(0.2)\n",
    "            prob_selection=softmax(prob_selection)\n",
    "            selected_index = np.random.choice(len(prob_selection), p=prob_selection)\n",
    "            if selected_index<= len(selected_slate)-1:\n",
    "                selected_item=selected_slate[selected_index]\n",
    "            else:\n",
    "                selected_item=0 #None\n",
    "            \n",
    "            \n",
    "            next_state = transition_function(current_state, selected_item)\n",
    "            \n",
    "            # Assume a reward for the transition (you should replace this with the actual reward from your environment)\n",
    "            reward_value = reward(current_state,selected_item) # Replace with the actual reward\n",
    "            \n",
    "            # Update the Q-value based on the transition\n",
    "            Q_item,Q_slate = update_q_value(Q_item,Q_slate, current_state,selected_item,selected_slate, reward_value, next_state,alpha, gamma)\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # Terminate the episode if a terminal state is reached (you should replace this with your termination condition)\n",
    "            if current_state in [-1, 1]:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "run_episodes(Q_item,Q_slate, transition_function, reward, update_q_value, num_episodes, alpha, gamma, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "For user state -1: optimal slate is (-1, -1, -1, -1) and (1, -1, 1, -1) \n",
      "4 7\n",
      "For user state 0: optimal slate is (-1, 1, -1, -1) and (-1, 1, 1, 1) \n",
      "11 15\n",
      "For user state 1: optimal slate is (1, -1, 1, 1) and (1, 1, 1, 1) \n"
     ]
    }
   ],
   "source": [
    "first_max = np.argmax(Q_slate, axis=1)\n",
    "second_max= np.argsort(Q_slate, axis=1)[:,-2]\n",
    "# Print the result\n",
    "for i in range(len(first_max)):\n",
    "    print(first_max[i],second_max[i])\n",
    "    print(f\"For user state {user_states[i]}: optimal slate is {slates[first_max[i]]} and {slates[second_max[i]]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.40734227e+04, 2.11474793e+02, 2.40398698e+02, 1.65709909e+02,\n",
       "        1.91114850e+02, 1.76178796e+02, 1.52080637e+02, 8.43890435e+01,\n",
       "        1.67178553e+02, 1.40993122e+02, 2.56452337e+02, 1.04495324e+02,\n",
       "        2.01043256e+02, 1.14349522e+02, 7.71175986e+01, 4.09108976e+01],\n",
       "       [2.19980087e+02, 2.57114794e+02, 2.32703441e+02, 1.48111158e+02,\n",
       "        3.08970647e+04, 2.24953735e+02, 3.12545093e+02, 4.19739941e+02,\n",
       "        2.95330700e+02, 2.49570273e+02, 2.85267109e+02, 3.65991398e+02,\n",
       "        1.23573753e+02, 2.01393826e+02, 3.18940807e+02, 3.26545613e+02],\n",
       "       [1.17676345e+01, 5.28654424e+01, 3.63354302e+01, 9.34517637e+01,\n",
       "        5.96568259e+01, 1.22049569e+02, 7.68647489e+01, 9.68577743e+01,\n",
       "        6.57254789e+01, 8.48527591e+01, 5.36585065e+01, 1.62438912e+04,\n",
       "        1.07102987e+02, 9.79745791e+01, 1.12027817e+02, 1.81067819e+02]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999117, 7.97966925],\n",
       "       [8.99999158, 8.9996102 ],\n",
       "       [7.99997791, 9.99965739]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6       , 0.21449518],\n",
       "       [0.32792112, 0.6       ],\n",
       "       [0.04852569, 0.40875872]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_i_u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
